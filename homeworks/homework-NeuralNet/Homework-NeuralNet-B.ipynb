{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part B (55 points)\n",
    "## Gradient descent for simple two and three layer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, Feb. 13, 2023.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment implements the gradient descent algorithm for a simple artificial neuron. The second part implements backpropagation for a simple network with one hidden unit.\n",
    "\n",
    "In the first part, the neuron will learn to compute logical OR. The neuron model and logical OR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_OR.jpeg\" style=\"width: 350px;\"/>\n",
    "\n",
    "This assignment requires some basic PyTorch knowledge. You can review your notes from lab and this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). The \"Introduction to PyTorch\" section on the PyTorch website is also helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `torch.tensor` objects for representing the data matrix `X` with targets `Y_or` (for the logical OR function). Each row of `X` is a different input pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = [[0.,0.], [0.,1.], [1.,0.], [1.,1.]]\n",
    "X = torch.tensor(X_list)\n",
    "Y_or = torch.tensor([0.,1.,1.,1.])\n",
    "N = X.shape[0] # number of input patterns\n",
    "print(\"Input tensor X:\")\n",
    "print('  has shape',X.shape)\n",
    "print('  and contains',X)\n",
    "print('Target tensor Y:')\n",
    "print('  has shape',Y_or.shape)\n",
    "print('  and contains',Y_or)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neuron operates as follows. Given an input vector $x$ (which is one row of input tensor $X$), the net input ($\\textbf{net}$) to the neuron is computed as follows\n",
    "\n",
    "$$ \\textbf{net} = \\sum_i x_i w_i + b,$$\n",
    "\n",
    "for weights $w_i$ and bias $b$. The activation function $g(\\textbf{net})$ is the logistic function\n",
    "\n",
    "$$ g(\\textbf{net}) = \\frac{1}{1+e^{-\\textbf{net}}},$$\n",
    "\n",
    "which is used to compute the predicted output $\\hat{y} = g(\\textbf{net})$. Finally, the loss (squared error) for a particular pattern $x$ is defined as \n",
    "\n",
    "$$ E(w,b) = (\\hat{y}-y)^2,$$\n",
    "\n",
    "where the target output is $y$. **Your main task is to manually compute the gradients of the loss $E$ with respect to the neuron parameters:**\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}, \\frac{\\partial E(w,b)}{\\partial b}.$$\n",
    "\n",
    "By manually, we mean to program the gradient computation directly, using the formulas discussed in class. This is in contrast to using PyTorch's `autograd` (Automatric differentiation) that computes the gradient automatically, as discussed in class, lab, and in the PyTorch tutorial (e.g., `loss.backward()`). First, let's write the activation function and the loss in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_logistic(net):\n",
    "    return 1. / (1.+torch.exp(-net))\n",
    "\n",
    "def loss(yhat,y):\n",
    "    return (yhat-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll also write two functions for examining the internal operations of the neuron, and the gradients of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forward(x,yhat,y):\n",
    "    # Examine network's prediction for input x\n",
    "    print(' Input: ',end='')\n",
    "    print(x.numpy())\n",
    "    print(' Output: ' + str(round(yhat.item(),3)))\n",
    "    print(' Target: ' + str(y.item()))\n",
    "\n",
    "def print_grad(grad_w,grad_b):\n",
    "    # Examine gradients\n",
    "    print('  d_loss / d_w = ',end='')\n",
    "    print(grad_w)\n",
    "    print('  d_loss / d_b = ',end='')\n",
    "    print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive in and begin the implementation of stochastic gradient descent. We'll initialize our parameters $w$ and $b$ randomly, and proceed through a series of epochs of training. Each epoch involves visiting the four training patterns in random order, and updating the parameters after each presentation of an input pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (10 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code to manually compute the gradient in closed form.\n",
    "    <ul>\n",
    "        <li>See lecture slides for the equation for the gradient for the weights w.</li>\n",
    "        <li>Derive (or reason) to get the equation for the gradient for bias b.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (5 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code for the weight and bias update rule for gradient descent.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the neuron's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2, requires_grad=True) # [size 2] tensor\n",
    "b = torch.randn(1, requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # get one input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x_pat,w)+b\n",
    "        yhat = g_logistic(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.         \n",
    "            raise Exception('Replace with your code.')                      \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# print a final pass through patterns\n",
    "for p in range(X.shape[0]):\n",
    "    x_pat = X[p]\n",
    "    net = torch.dot(x_pat,w)+b\n",
    "    yhat = g_logistic(net)\n",
    "    y = Y_or[p]\n",
    "    print(\"Final result:\")\n",
    "    print_forward(x_pat,yhat,y)\n",
    "    print(\"\")\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (logistic activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change the activation function to \"linear\" (identity function) from the \"logistic\" function, such that $g(\\textbf{net}) = \\textbf{net}$. With a linear rather than logistic activation, the output will no longer be constrained between 0 and 1. The artificial neuron will still try to solve the problem with 0/1 targets. Here is the simple implementation of $g(\\cdot)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (5 points) </h3>\n",
    "<br>\n",
    "Just as before, fill in the missing code fragments for implementing gradient descent. This time we are using the linear activation function. Be sure to change your gradient calculation to reflect the new activation function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2, requires_grad=True) # [size 2] tensor\n",
    "b = torch.randn(1, requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # get one input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x_pat,w)+b\n",
    "        yhat = g_linear(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.         \n",
    "            raise Exception('Replace with your code.')                      \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            #    w -=   ....\n",
    "            #    b -=   ....\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# print a final pass through patterns\n",
    "for p in range(X.shape[0]):\n",
    "    x_pat = X[p]\n",
    "    net = torch.dot(x_pat,w)+b\n",
    "    yhat = g_linear(net)\n",
    "    y = Y_or[p]\n",
    "    print(\"Final result:\")\n",
    "    print_forward(x_pat,yhat,y)\n",
    "    print(\"\")\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (linear/null activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (10 points) </h3>\n",
    "<br>\n",
    "You'll see above that the artificial neuron, with the simple linear (identity) activation, does worse on the OR problem. Examine the learned weights and bias, and explain why the network does not arrive at a perfect solution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we have a simple multi-layer network with two input neurons, one hidden neuron, and one output neuron. Both the hidden and output unit should use the logistic activation function. We will learn to compute logical XOR. The network and logical XOR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_XOR.jpeg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (15 points) </h3>\n",
    "<br>\n",
    "You will implement backpropagation for this simple network. In the code below, you have several parts to fill in. First, define the forward pass to compute the output `yhat` from the input `x`. Second, fill in code to manually compute the gradients for all five weights w and two biases b in closed form. Third, fill in the code for updating the biases and weights.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the network's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same input tensor X and new labels y for xor\n",
    "Y_xor = torch.tensor([0.,1.,1.,0.])\n",
    "N = X.shape[0] # number of input patterns\n",
    "\n",
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w_34 = torch.randn(2,requires_grad=True) # [size 2] tensor representing [w_3,w_4]\n",
    "w_012 = torch.randn(3,requires_grad=True) # [size 3] tensor representing [w_0,w_1,w_2]\n",
    "b_0 = torch.randn(1,requires_grad=True) # [size 1] tensor\n",
    "b_1 = torch.randn(1,requires_grad=True) # [size 1] tensor\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 10000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x_pat = X[p,:] # input pattern\n",
    "        \n",
    "        # Compute the output of hidden neuron h\n",
    "        # e.g., two lines like the following\n",
    "        #  net_h = ...\n",
    "        #  h = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')                  \n",
    "        \n",
    "        # Compute the output of neuron yhat\n",
    "        # e.g., two lines like the following\n",
    "        #  net_y = ...\n",
    "        #  yhat = ...\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')                     \n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_xor[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x_pat,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x_pat,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  should include at least these 4 lines (helper lines may be useful)\n",
    "            #    w_34_grad = ...  \n",
    "            #    b_0_grad = ...\n",
    "            #    w_012_grad = ...\n",
    "            #    b_1_grad = ...\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.\n",
    "            raise Exception('Replace with your code.')                      \n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34_grad.numpy(),b_0_grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012_grad.numpy(),b_1_grad.numpy())\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34.grad.numpy(),b_0.grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012.grad.numpy(),b_1.grad.numpy())\n",
    "            print(\"\")\n",
    "        w_34.grad.zero_() # clear PyTorch's gradient\n",
    "        b_0.grad.zero_()\n",
    "        w_012.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            # Four lines of the form\n",
    "            # w_34 -= ...\n",
    "            # b_0 -= ...\n",
    "            # w_012 -= ...\n",
    "            # b_1 -= ...\n",
    "            raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "\n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (XOR)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 6 (10 points) </h3>\n",
    "<br>\n",
    "After running your XOR network, print the values of the learned weights and biases. Your job now is to describe the solution that the network has learned. How does it work? Walk through each input pattern to describe how the network computes the right answer (if it does). See discussion in lecture for an example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
