{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Further reading:</h4><p>This notebook is adapted from the <a href=\"https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\">PyTorch: A 60 Minute Blitz</a> tutorial on the PyTorch website. For documentation and more tutorials, visit <a href=\"https://pytorch.org\">pytorch.org</a></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Autograd\n",
    "\n",
    "``torch.autograd`` is PyTorch’s automatic differentiation engine for training neural networks. This notebook will help you build a conceptual understanding of how autograd works.\n",
    "\n",
    "## Background\n",
    "Neural networks (NNs) are collections of nested functions that are executed on some input data. These functions are defined by *parameters* (consisting of weights and biases), which in PyTorch are stored in tensors (see the previous notebook for more on tensors).\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "**Forward Propagation**: In forward prop, the NN runs the input data through each of its functions to generate its output. This output might be something like a guess for whether an input image is of a cat, a dog, etc.\n",
    "\n",
    "**Backward Propagation**: In backprop, the NN adjusts its parameters proportionate to the error in its output. It does this by traversing backwards, starting from the output and moving toward the input, collecting the derivatives of the error with respect to the parameters of the functions (*gradients*), and optimizing the parameters using gradient descent. For a more detailed walkthrough of backprop, check out [this video](https://www.youtube.com/watch?v=tIeHLnjs5U8) from Grant Sanderson (3Blue1Brown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation using Autograd\n",
    "Let's take a look at how ``autograd`` collects gradients with a very simple example. We'll create a 1x1 tensor ``x`` and set ``requires_grad=True``. This signals to ``autograd`` that every operation on ``x`` should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make another tensor, ``y``, that's a function of ``x``:\n",
    "\n",
    "$$y = x^3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ``y`` has a ``grad_fn`` attribute. The gradient here is just a derivative:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = 3x^2 $$\n",
    "\n",
    "And we can check if autograd did its job correctly by calling ``.backward()`` on ``y``, which will store the gradient in x.grad. We expect that to be the same as $3x^2$—is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "if x.grad == 3 * x ** 2:\n",
    "    print(\"Gradients match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a slightly more complex example: Let's say we have a tensor ``a``, which represents the second-to-last hidden layer of a neural net, ``b``, which represents the last hidden layer, and ``y_hat``, which represents the output. These will all be 1x3 tensors this time (rather than a scalars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "b = a ** 3\n",
    "b.retain_grad()\n",
    "y_hat = b ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll say for simplicity that the error is just the sum of the elements of $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = torch.sum(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling ``.backward`` on ``error``, its gradients with respect to ``a`` and ``b`` should be stored in ``a.grad`` and ``b.grad``. Are they what we'd expect?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}}{\\partial b} &= 2b \\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial a} &= \\frac{\\partial \\hat{y}}{\\partial b} \\frac{\\partial b}{\\partial a} = (2b)(3a^2) = 6ba^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.backward()\n",
    "if all(a.grad == 6 * b * a ** 2) and all(b.grad == 2 * b):\n",
    "    print(\"Gradients match!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
