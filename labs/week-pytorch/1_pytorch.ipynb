{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Further reading:</h4><p>This notebook is adapted from the <a href=\"https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\">PyTorch: A 60 Minute Blitz</a> tutorial on the PyTorch website. For documentation and more tutorials, visit <a href=\"https://pytorch.org\">pytorch.org</a></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch\n",
    "\n",
    "PyTorch is a free, open-source machine learning framework with a Python interface that's widely used both in research and industry. For the purposes of this class, it lets us easily set up and train neural network models. These lab notebooks are just a brief introduction to PyTorch, but will hopefully get you comfortable with the terms and tools that will be important, starting with perhaps the most fundamental: tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tensors\n",
    "\n",
    "Tensors are a data structure—a way of storing numbers. They are very similar to arrays, which you may have encountered before (such as the ndarrays in NumPy). A tensor can be as simple as a one-dimensional list of numbers:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "But a tensor can also be a 2-dimensional array of numbers:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or it can have 3, 4, or any other number of dimensions. You might talk about a 1x3 tensor (like the first example above) or a 3x3 tensor (like the second example) or an Nx1x32x32 tensor (which we'll see in a later notebook). So, at least as far as the field of machine learning is concerned, a tensor is just a big block of numbers—a multidimensional array.\n",
    "\n",
    "PyTorch uses tensors to store the data that make up the input to a neural network model (e.g. images for classification), and also uses them to store the parameters that define the model (its weights and biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Tensors\n",
    "\n",
    "There are several ways to initialize a tensor—let's look at some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "data_tensor = torch.tensor(data)\n",
    "print(data_tensor, data_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays. Note that the new tensor and the NumPy array will share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "data_tensor = torch.from_numpy(np_array)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With random or constant values**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using another tensor as a template**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "x_ones = torch.ones_like(data_tensor) # retains the properties of data_tensor\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(data_tensor, dtype=torch.float) # overrides the datatype of data_tensor\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Attributes\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations\n",
    "\n",
    "A full list of tensor operations can be found [here](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic (element-wise)**\n",
    "\n",
    "You can use the usual +, -, \\*, / symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(10, (3, 2))\n",
    "y = torch.randint(10, (3, 2))\n",
    "print(x, '\\n')\n",
    "print(y, '\\n')\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [-1, 0]])\n",
    "y = torch.tensor([[4, 0], [0, 4]])\n",
    "print(x, '\\n')\n",
    "print(y, '\\n')\n",
    "z = x @ y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard numpy-like indexing and slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.zeros(3, 3)\n",
    "print(tensor, '\\n')\n",
    "tensor[0, 0] = 3\n",
    "tensor[:, 2] = 5\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-place operations**\n",
    "\n",
    "Operations that have a ``_`` suffix are in-place. For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(10)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In-place operations save some memory, but can potentially overwrite values required to compute gradients, so their use is discouraged.</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors**\n",
    "\n",
    "You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension. See also [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html), another operation that is subtly different from ``torch.cat``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing device**\n",
    "\n",
    "Typically, tensor operations will be faster on a GPU (if supported) than a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(18_000, 18_000)\n",
    "y = torch.rand(18_000, 18_000)\n",
    "start_time = time.time()\n",
    "\n",
    "# this should take several seconds to run:\n",
    "z = x @ y\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(np.round(elapsed_time, 3), \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move our tensor to the GPU if available\n",
    "\n",
    "# CUDA: Nvidia GPUs\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to('cuda')\n",
    "    y = y.to('cuda')\n",
    "    \n",
    "# MPS: Mac GPUs\n",
    "if torch.backends.mps.is_available():\n",
    "    x = x.to('mps')\n",
    "    y = y.to('mps')\n",
    "    \n",
    "print(f\"Tensor is stored on: {x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the tensors were moved to a GPU, this should be faster\n",
    "z = 0\n",
    "start_time = time.time()\n",
    "z = x @ y\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(np.round(elapsed_time, 3), \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
