{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Bayesian modeling - Part A (100 points)\n",
    "## Bayesian concept learning with the number game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement (mostly from scratch!) a Bayesian concept learning model and the \"number game,\" as covered in lecture. As with so many of our everyday inferences, the data we receive is far too sparse and noisy to be conclusive. Nevertheless, people must make generalizations and take actions based on imperfect and insufficient data. In data science and machine learning, the situation is often the same: the data is not enough to produce an answer with certainty, yet we can make meaningful generalizations anyway. What computational mechanisms can support these types of inferences?\n",
    "\n",
    "The number game is a quintessential inductive problem. In the number game, there is an unknown computer program that generates numbers in the range 1 to 100. You are provided with a small set of random examples from this program. For instance, in the figure below, you get two random examples from the program: the numbers '8' and '2'.\n",
    "\n",
    "<img src=\"images/number_game_comp.jpeg\" style=\"width: 300px;\"/>\n",
    "\n",
    "Which numbers will also be accepted by the same program? Of course, it depends what the program is, and you don't have enough information to be sure. Should '9' be accepted? Perhaps, if the concept is \"all numbers up to 10.\" What about '10'? A better candidate, since the program could again be \"numbers up to 10\", or \"all even numbers.\" What about '16'? This is another good candidate, and the program \"powers of 2\" is also consistent with the examples so far. How should one generalize based on the evidence so far? This homework explores how the Bayesian framework provides an answer to this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3>You should read the following paper carefully.</h3>\n",
    "Josh Tenenbaum's paper introduced the number game. You can download the paper on EdStem:\n",
    "<ul>\n",
    "<li>Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in Neural Information Processing Systems (NIPS).</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian model\n",
    "In the number game, we receive a set of $n$ positive examples $X = \\{x^{(1)},...,x^{(n)}\\}$ of an unknown concept $C$. In a Bayesian analysis of the task, the goal is predict $P(y \\in C\\ |\\ X)$, which is the probability that a new number $y$ is also a member of the concept $C$ after receiving the set of examples $X$.\n",
    "\n",
    "#### Updating beliefs with Bayes' rule\n",
    "Let's proceed with the Bayesian model of the task. There is a hypothesis space $H$ of concepts, where a particular member of the hypothesis space (i.e., a particular concept) is denoted $h \\in H$. The Bayesian model includes a prior distribution $P(h)$ over the hypotheses and a likelihood $P(X|h)$. Bayes' rule specifies how to compute the posterior distribution over hypotheses given these two pieces:\n",
    "\\begin{equation}\n",
    "P(h|X) = \\frac{P(X|h)P(h)}{\\sum_{h' \\in H} P(X|h')P(h')}\n",
    "\\end{equation}\n",
    "The likelihood and prior are specified below.\n",
    "\n",
    "#### Likelihood\n",
    "We assume that each number in $X$ is an independent sample from the set of all valid numbers. Thus, the likelihood decomposes as a product of individual probabilities,\n",
    "\\begin{equation}\n",
    "P(X|h) = \\prod_{i=1}^n P(x^{(i)}|h).\n",
    "\\end{equation}\n",
    "We assume that the numbers are sampled uniformly at random from the set of valid numbers, such that $P(x^{(i)}|h) = \\frac{1}{|h|}$ if $x^{(i)} \\in h$ and $P(x^{(i)}|h) = 0$ otherwise. The term $|h|$ is the cardinality or set size of the hypothesis $h$.\n",
    "\n",
    "#### Prior\n",
    "The hypothesis space $H$ includes two main kinds of hypotheses. You can think of each hypothesis as a list of the numbers that fit that hypothesis.\n",
    "- The first kind consists of mathematical hypotheses such as odd numbers, even numbers, square numbers, cube numbers, primes, multiples of $n$, powers of $n$, and numbers ending with a particular digit. Each mathematical hypothesis is given equal weight in the prior.\n",
    "- The second kind consists of interval hypotheses, which are solid intervals of numbers, such as $12, 13, 14, 15, 16, 17$. Intervals of intermediate size are favored (rather than very small or large hypotheses) by reweighting according to an Erlang distribution, $P(h) \\propto \\frac{|h|}{\\sigma^2} \\exp{-|h|/\\sigma}$ such that $\\sigma=10$.\n",
    "\n",
    "There is a free parameters `mylambda` controls how much of the prior is specified by each type of hypothesis, with `mylambda` weight going to the mathematical hypotheses and `1-mylambda` weights going to the interval hypotheses.\n",
    "\n",
    "We provide starter code below that generates the mathematical hypotheses and their prior probabilities (in natural log space).\n",
    "\n",
    "#### Making Bayesian predictions\n",
    "Once we have the posterior beliefs over hypotheses, we want to be able to make predictions about the membership of a new number $y$ in the concept $C$, or as mentioned $P(y \\in C\\ |\\ X)$. To compute this, we average over all possible hypotheses weighted by the posterior probability,\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\in C\\ |\\ X) = \\sum_{h \\in H} P(y \\in C\\ |\\ h) P(h|X),\n",
    "\\end{equation}\n",
    "\n",
    "where the first term is simply $1$ or $0$ based on the membership of $y$ in h, and the second term is the posterior weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some packaegs that may be useful\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "x_max = 100 # (numbers between 1 and 100 are allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four examples of math hypotheses:\n",
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n",
      "\n",
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]\n",
      "\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "\n",
      "[1, 8, 27, 64]\n",
      "\n",
      "Their prior log-probabilities:\n",
      "[-3.93182563 -3.93182563 -3.93182563 -3.93182563]\n"
     ]
    }
   ],
   "source": [
    "# Generate a list of all mathematical hypotheses\n",
    "def make_h_odd():\n",
    "    return list(range(1,x_max+1,2))\n",
    "\n",
    "def make_h_even():\n",
    "    return list(range(2,x_max+1,2))\n",
    "\n",
    "def make_h_square():\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x**2 <= x_max:\n",
    "            h.append(x**2)\n",
    "    return h\n",
    "\n",
    "def make_h_cube():\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x**3 <= x_max:\n",
    "            h.append(x**3)\n",
    "    return h\n",
    "\n",
    "def make_h_primes():\n",
    "    return [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
    "\n",
    "def make_h_mult_of_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x*y <= x_max:\n",
    "            h.append(x*y)\n",
    "    return h\n",
    "\n",
    "def make_h_powers_of_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if y**x <= x_max:\n",
    "            h.append(y**x)\n",
    "    return h\n",
    "\n",
    "def make_h_numbers_ending_in_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if str(x)[-1] == str(y):\n",
    "            h.append(x)\n",
    "    return h\n",
    "\n",
    "def generate_math_hypotheses(mylambda):\n",
    "    h_set = [make_h_odd(), make_h_even(), make_h_square(), make_h_cube(), make_h_primes()]\n",
    "    h_set += [make_h_mult_of_y(y) for y in range(3,13)]\n",
    "    h_set += [make_h_powers_of_y(y) for y in range(2,11)]\n",
    "    h_set += [make_h_numbers_ending_in_y(y) for y in range(0,10)]\n",
    "    n_hyp = len(h_set)\n",
    "    log_prior = np.log(mylambda * np.ones(n_hyp) / float(n_hyp))\n",
    "    return h_set, log_prior\n",
    "\n",
    "h_set_math, log_prior_math = generate_math_hypotheses(2./3)\n",
    "print(\"Four examples of math hypotheses:\")\n",
    "for i in range(4):\n",
    "    print(h_set_math[i])\n",
    "    print(\"\")\n",
    "print(\"Their prior log-probabilities:\")\n",
    "print(log_prior_math[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four examples of interval hypotheses\n",
      "[1]\n",
      "\n",
      "[1, 2]\n",
      "\n",
      "[1, 2, 3]\n",
      "\n",
      "[1, 2, 3, 4]\n",
      "\n",
      "Their prior log-probabilities:\n",
      "[-10.197254    -9.60410682  -9.29864171  -9.11095964]\n"
     ]
    }
   ],
   "source": [
    "## Generate a list of all interval hypotheses\n",
    "def make_h_between_y_and_z(y,z):\n",
    "    assert(y >= 1 and z <= x_max)\n",
    "    return list(range(y,z+1))\n",
    "\n",
    "def pdf_erlang(x,sigma=10.):\n",
    "    return (x / sigma**2) * np.exp(-x/sigma)\n",
    "\n",
    "def generate_interval_hypotheses(mylambda):\n",
    "    h_set = []\n",
    "    for y in range(1,x_max+1):\n",
    "        for z in range(y,x_max+1):            \n",
    "            h_set.append(make_h_between_y_and_z(y,z))\n",
    "    nh = len(h_set)\n",
    "    pv = np.ones(nh)\n",
    "    for idx,h in enumerate(h_set): # prior based on length\n",
    "        pv[idx] = pdf_erlang(len(h))\n",
    "    pv = pv / np.sum(pv)\n",
    "    pv = (1-mylambda) * pv\n",
    "    log_prior = np.log(pv)\n",
    "    return h_set, log_prior\n",
    "\n",
    "h_set_int, log_prior_int = generate_interval_hypotheses(2./3)\n",
    "print(\"Four examples of interval hypotheses\")\n",
    "for i in range(4):\n",
    "    print(h_set_int[i])\n",
    "    print(\"\")\n",
    "print(\"Their prior log-probabilities:\")\n",
    "print(log_prior_int[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human behavioral judgments\n",
    "Tenenbaum ran eight participants in an experiment where they were provided with various sets $X$ of random positive examples from a concept. They were asked to rate the probability that each of 30 test numbers would belong to the same concept of the observed examples. \n",
    "\n",
    "The following plot shows the mean rating across the human participants for three different sets. Note that since only 30 test numbers were evaluated, and thus a value of 0 in the plot indicates missing data (rather than zero probability).\n",
    "<img src=\"images/number_game_human.jpeg\" style=\"width: 800px;\"/>\n",
    "\n",
    "Your goal is to implement the Bayesian concept learning model in order to produce the same plots, although with the model judgements rather than the human judgements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 30 test numbers that Tenenbaum used are here\n",
    "x_eval = [2,4,6,8,9,10]+list(range(12,23))+[24,25,26,28,32,36,41,56,62,64,87,95,96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation \n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (90 points) </h3>\n",
    "<br>\n",
    "Your main task is to produce the same three plots as shown above, although showing model predictions rather than human behavioral judgements. To do so, you'll need to implement the Bayesian concept learning model. A successful implementation will include the following components:\n",
    "<ul>\n",
    "    <li>A function for computing the log-probability of a hypothesis h according to the prior (largely provided in starter code).</li>\n",
    "    <li>A function for computing the log-likelihood of a set of numbers X given a particular hypothesis h.</li>\n",
    "    <li>A function for computing the log-posterior over all hypotheses h given a set of numbers X that were sampled from h.</li>\n",
    "    <li>According to the \"Making Bayesian predictions\" section above, a function for computing the probability that a new number y belongs to the same concept as a set of sampled numbers X</li>\n",
    "    <li>Code for making the plots</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Tip: For probabilistic modeling in general, we like to compute probabilities in log-space to help avoid numerical issues such as underflow. For instance, rather than multiplying the prior and likelihood (resulting in potentially very small numbers), we sum the log-prior and the log-likelihood. Also, check out the nifty `logsumexp` function ([see scipy doc](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html)) which is used to normalize log-probability distributions in a numerically safer way. This function is already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (10 points) </h3>\n",
    "<br>\n",
    "Discuss your general thoughts on this Bayesian model to understand human judgments in the number game. Discussion questions could include the following (as well as others):\n",
    "<ul>\n",
    "    <li>Is the model convincing? Why or why not?</li>\n",
    "    <li>Is the number game and Bayesian model relevant to more naturalistic settings for concept learning in childhood or everyday life?</li>\n",
    "    <li>Where could the hypothesis space come from?</li>\n",
    "    <li>What algorithms could people be using to approximate Bayesian inference, rather than enumerating all the hypotheses, as in the current implementation?</li>\n",
    "</ul>\n",
    "<br>\n",
    "Please write a short response in the cell below. Your response should be about two paragraphs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
